# -*- coding: utf-8 -*-
"""EnglishAssign1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nXuJPw0o9FQ9Zo4svzL-ypH35pKL9_p3
"""

!pip install nltk

import nltk
nltk.download('popular')

from nltk.tokenize import sent_tokenize, word_tokenize

"""For Processing file from drive steps:

!pip install -U -q PyDrive

from pydrive.auth import GoogleAuth

from pydrive.drive import GoogleDrive

from google.colab import auth

from oauth2client.client import GoogleCredentials

auth.authenticate_user()

gauth = GoogleAuth()

gauth.credentials = GoogleCredentials.get_application_default()

drive = GoogleDrive(gauth)

downloaded = drive.CreateFile({'id':'1XbsoF7-1dgSCJxw8D-MWlOTHZGZSr34o'}) 
// replace the id with id of file you want to access
downloaded.GetContentFile('en_wiki.txt') 

"""

with open('/content/drive/My Drive/en_wiki.txt') as f:
  text = f.read()

"""1st Method of Sentence segmentation"""

sentenceTokens = sent_tokenize(text)
sentenceTokens[:100]

"""2nd Method of Sentence segmentation"""

from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters

tokenizer = PunktSentenceTokenizer()
tokenizer.train(text)
tokenizer._params.abbrev_types
sentenceTokens2 = tokenizer.tokenize(text)
sentenceTokens2[:100]

1st Method of Word Tokenization

wordTokens1 = word_tokenize(text)
wordTokens1[:400]

2nd Method of word Tokenization

from nltk.tokenize import TreebankWordTokenizer 
tokenizer1 = TreebankWordTokenizer() 
wordTokens2 = tokenizer1.tokenize(text) 
wordTokens2[:400]

"""Analysing ngrams"""

from nltk.util import ngrams
from collections import Counter

my_unigrams = list(ngrams(wordTokens1,1))
my_unigrams[:50]

"""Bigrams:-"""

my_bigrams = list(ngrams(wordTokens1,2))
my_bigrams[:50]

"""trigrams:-"""

my_trigrams = list(ngrams(wordTokens1,3))
my_trigrams[:50]

"""Calculating Frequency Distribution"""

import itertools
import operator
import collections

"""Unigrams Freqdist

Most Frequent Printed
"""

fdistUni = nltk.FreqDist(my_unigrams)
sortedUni = dict(sorted(fdistUni.items(), key=operator.itemgetter(1),reverse=True))
top10Uni = dict(itertools.islice(sortedUni.items(), 10)) 
print(top10Uni)

"""Least Frequent Printed"""

acensortedUni = dict(sorted(fdistUni.items(), key=operator.itemgetter(1)))
below10Uni = dict(itertools.islice(acensortedUni.items(), 10)) 
print(below10Uni)

"""Bigrams FreqDist

Most Frequent Printed
"""

fdistBi = nltk.FreqDist(my_bigrams)
sortedBi = dict(sorted(fdistBi.items(), key=operator.itemgetter(1),reverse=True))
top10Bi = dict(itertools.islice(sortedBi.items(), 10)) 
print(top10Bi)

"""Least Frequent Printed"""

acensortedBi = dict(sorted(fdistBi.items(), key=operator.itemgetter(1)))
below10Bi = dict(itertools.islice(acensortedBi.items(), 10)) 
print(below10Bi)

"""Trigrams FreqDist

Most Frequent Printed
"""

fdistTri = nltk.FreqDist(my_trigrams)
sortedTri = dict(sorted(fdistTri.items(), key=operator.itemgetter(1),reverse=True))
top10Tri = dict(itertools.islice(sortedTri.items(), 10)) 
print(top10Tri)

"""Least Frequent Printed"""

acensortedTri = dict(sorted(fdistTri.items(), key=operator.itemgetter(1)))
below10Tri = dict(itertools.islice(acensortedTri.items(), 10)) 
print(below10Tri)

import matplotlib.pyplot as plt

len(sortedUni)

topUni = dict(itertools.islice(sortedUni.items(), 395757))

freqUni = list(topUni.values())

"""Calculting All Unigrams in corpus"""

total = 0
for val in freqUni:
  total = total + val
print(total)

"""Finding most frequent unigrams to cover 90% corpus"""

nintyPercent = total*(9/10)
calcu=0
count=0
for val in freqUni:
  count = count +1
  calcu = calcu + val
  if calcu>nintyPercent:
    break
print(count)

freqUni[:1]

bins = [1,2,3,4,5,6,7,8,9,10,15,20,25,30,35,40,50]

plt.hist(freqUni,bins=bins,edgecolor='black')

import numpy as np
from sklearn.linear_model import LinearRegression

x_cordinate = [i+1 for i in range(len(sortedUni))]
y_cordinate = list(sortedUni.values())


x_cordinate = np.log(np.array(x_cordinate).reshape(-1, 1))
y_cordinate = np.log(np.array(y_cordinate).reshape(-1, 1))

lr  = LinearRegression()
lr.fit(x_cordinate,y_cordinate)
print('Intercept :',lr.intercept_)
print('Slope :',lr.coef_)

plt.xlabel("log(Rank)")
plt.ylabel("log(Frequency)")
plt.title("Frquency Distribution")
plt.plot(x_cordinate, y_cordinate)
plt.show()

"""Bigram Distribution"""

len(sortedBi)

topBi = dict(itertools.islice(sortedBi.items(), 4095732))

freqBi = list(topBi.values())

"""Finding to total bigrams"""

total = 0
for val in freqBi:
  total = total + val
print(total)

"""Calculating Most Frequent for 80% coverage"""

eightyPercent = total*(8/10)
calcu=0
count=0
for val in freqBi:
  count = count +1
  calcu = calcu + val
  if calcu>eightyPercent:
    break
print(count)

freqBi[:2000]

binb = [1,2,3,4,5,6,7,8,9,10,15,20,25,30,35,40,50]
plt.hist(freqBi,bins=binb,edgecolor='black')

len(sortedTri)

topTri = dict(itertools.islice(sortedTri.items(), 10827502))

freqTri = list(topTri.values())

"""Finding to total Trigrams"""

total = 0
for val in freqTri:
  total = total + val
print(total)

"""Calculating no. of most frequent to cover 70%"""

seventyPercent = total*(7/10)
calcu=0
count=0
for val in freqTri:
  count = count +1
  calcu = calcu + val
  if calcu>seventyPercent:
    break
print(count)

freqTri[:20]

bint = [1,2,3,4,5,6,7,8,9,10,15,20,25,30,35,40,50]
plt.hist(freqTri,bins=bint,edgecolor='black')

"""Stemming Tokens"""

from nltk.stem import PorterStemmer
ps = PorterStemmer()

stemmedWords = []
for w in wordTokens1:
    stemmedWords.append(ps.stem(w))

stemmedWords[:20]

"""Unigrams from stemmed tokens"""

stemmy_unigrams = list(ngrams(stemmedWords,1))
stemmy_unigrams[:10]

"""Bigrams from stemmed Tokens"""

stemmy_bigrams = list(ngrams(stemmedWords,2))
stemmy_bigrams[:10]

"""Trigrams from stemmed tokens"""

stemmy_trigrams = list(ngrams(stemmedWords,3))
stemmy_trigrams[:10]

"""Stemmed Unigrams  Frequency Analysis"""

fdistStemUni = nltk.FreqDist(stemmy_unigrams)
sortedStemUni = dict(sorted(fdistStemUni.items(), key=operator.itemgetter(1),reverse=True))
top10StemUni = dict(itertools.islice(sortedStemUni.items(), 10)) 
print(top10StemUni)

"""Least Frequent Printed"""

acensortedStemUni = dict(sorted(fdistStemUni.items(), key=operator.itemgetter(1)))
below10StemUni = dict(itertools.islice(acensortedStemUni.items(), 10)) 
print(below10StemUni)

len(sortedStemUni)

freqStemUni = list(sortedStemUni.values())

"""Total no. of unigrams"""

total = 0
for val in freqStemUni:
  total = total + val
print(total)

"""Most frequent unigrams required for 90% coverage"""

nintyPercent = total*(9/10)
calcu=0
count=0
for val in freqStemUni:
  count = count +1
  calcu = calcu + val
  if calcu>nintyPercent:
    break
print(count)

binu = [1,2,3,4,5,6,7,8,9,10,15,20,25,30,35,40,50]
plt.hist(freqStemUni,bins=binu,edgecolor='black')

"""Stemmed Bigrams Frequency Analysis"""

fdistStemBi = nltk.FreqDist(stemmy_bigrams)
sortedStemBi = dict(sorted(fdistStemBi.items(), key=operator.itemgetter(1),reverse=True))
top10StemBi = dict(itertools.islice(sortedStemBi.items(), 10)) 
print(top10StemBi)

"""Least Frequent Printed"""

acensortedStemBi = dict(sorted(fdistStemBi.items(), key=operator.itemgetter(1)))
below10StemBi = dict(itertools.islice(acensortedStemBi.items(), 10)) 
print(below10StemBi)

"""Total distinct Bigrams"""

len(sortedStemBi)

freqStemBi = list(sortedStemBi.values())

"""Total no. of Bigrams"""

total = 0
for val in freqStemBi:
  total = total + val
print(total)

"""Most Frequent Bigrams required for 80% coverage"""

eightyPercent = total*(8/10)
calcu=0
count=0
for val in freqStemBi:
  count = count +1
  calcu = calcu + val
  if calcu>eightyPercent:
    break
print(count)

binb = [1,2,3,4,5,6,7,8,9,10,15,20,25,30,35,40,50]
plt.hist(freqStemBi,bins=binb,edgecolor='black')

"""Stemmed Trigram Frequency Analysis"""

fdistStemTri = nltk.FreqDist(stemmy_trigrams)
sortedStemTri = dict(sorted(fdistStemTri.items(), key=operator.itemgetter(1),reverse=True))
top10StemTri = dict(itertools.islice(sortedStemTri.items(), 10)) 
print(top10StemTri)

"""Least Frequent Trigrams Printed"""

acensortedStemTri = dict(sorted(fdistStemTri.items(), key=operator.itemgetter(1)))
below10StemTri = dict(itertools.islice(acensortedStemTri.items(), 10)) 
print(below10StemTri)

"""Total no. of Distinct Trigrams"""

len(sortedStemTri)

freqStemTri = list(sortedStemTri.values())

"""Total No. of Trigrams"""

total = 0
for val in freqStemTri:
  total = total + val
print(total)

"""Most Frequent Trigrams for 70% coverage"""

seventyPercent = total*(7/10)
calcu=0
count=0
for val in freqStemTri:
  count = count +1
  calcu = calcu + val
  if calcu>seventyPercent:
    break
print(count)

bint = [1,2,3,4,5,6,7,8,9,10,15,20,25,30,35,40,50]
plt.hist(freqStemTri,bins=bint,edgecolor='black')

"""**Hueristic Approach for word Tokenization**"""

import re

pronouns = "(I|He|She|It|They|Their|Our|We|That|This)"
AfterAphosetrophe = "(ll|m|s|d|ve|t)"
StartingCapital = "([A-Z][a-z]*)"
caps = "([A-Z])"
digits = "([0-9])"
lower = "([a-z])"
prefixes = "(Mr|St|Mrs|Ms|Dr|MR|MRS)[.]"
suffixes = "(Inc|Ltd|Jr|Sr|Co)"
caps = "([A-Z])"
starters = "(Mr|Mrs|Ms|Dr|He\s|She\s|It\s|They\s|Their\s|Our\s|We\s|But\s|However\s|That\s|This\s|Wherever)"
acronyms = "([A-Z][.][A-Z][.](?:[A-Z][.])?)"
websites = "[.](edu|com|net|org|io|gov)"
digits = "([0-9])" 
brackets = "(\(|\{|\"|\[|\]|\}|\))"

with open('/content/drive/My Drive/Test.txt') as f:
  text = f.read()

"""**Sentence Segmentation implementing hueristic**"""

text = " " + text + "  "
text = text.replace("\n"," ")
text = re.sub(prefixes,"\\1<use>",text)
text = re.sub(digits + "[.]" + digits,"\\1<prd>\\2",text)
text = re.sub(websites,"<use>\\1",text)
if "Ph.D" in text: text = text.replace("Ph.D.","Ph<prd>D<prd>")
text = re.sub("\s" + caps + "[.] "," \\1<use> ",text)
text = re.sub("\s" + StartingCapital + "[.] "," \\1<use> ",text)
text = re.sub("\s" + StartingCapital + "[']" + AfterAphosetrophe," \\1 ' \\2 ",text)
text = re.sub(acronyms+" "+starters,"\\1<stop> \\2",text)
text = re.sub(caps + "[.]" + caps + "[.]" + caps + "[.]","\\1<use>\\2<use>\\3<use>",text)
text = re.sub(caps + "[.]" + caps + "[.]","\\1<use>\\2<use>",text)
text = re.sub(lower + "[.]" + lower + "[.]","\\1<use>\\2<use>",text)

text = re.sub(lower + "[.]" + lower + "[.]"+ lower + "[.]","\\1<use>\\2<use>",text)
text = re.sub(" "+suffixes+"[.] "+starters," \\1<stop> \\2",text)
text = re.sub(" "+suffixes+"[.]"," \\1<use>",text)
text = re.sub(" " + caps + "[.]"," \\1<use>",text)


if "..." in text: text = text.replace("...","<prd><prd><prd>")
if "”" in text: text = text.replace(".”","”.")
if "\"" in text: text = text.replace(".\"","\".")
if "!" in text: text = text.replace("!\"","\"!")
if "?" in text: text = text.replace("?\"","\"?")

text = text.replace(".",".<stop>")
text = text.replace("?","?<stop>")
text = text.replace("!","!<stop>")
text = text.replace("<use>",".")
text = text.replace("<prd>",".")
sentences = text.split("<stop>")
print(sentences)

""" **Word Tokenization by implementing hueristic**"""

text = " " + text + "  "
text = text.replace("\n"," ")
text = re.sub(prefixes,"\\1<use>",text)
text = re.sub(digits + "[.]" + digits,"\\1<prd>\\2",text)
text = re.sub(websites,"<use>\\1",text)
if "Ph.D" in text: text = text.replace("Ph.D.","Ph<prd>D<prd>")
text = re.sub("\s" + caps + "[.] "," \\1<use> ",text)
text = re.sub("\s" + StartingCapital + "[.] "," \\1<use> ",text)
text = re.sub("\s" + StartingCapital + "[']" + AfterAphosetrophe," \\1 ' \\2 ",text)
text = re.sub(acronyms+" "+starters,"\\1<space> \\2",text)
text = re.sub(caps + "[.]" + caps + "[.]" + caps + "[.]","\\1<use>\\2<use>\\3<use>",text)
text = re.sub(caps + "[.]" + caps + "[.]","\\1<use>\\2<use>",text)
text = re.sub(lower + "[.]" + lower + "[.]","\\1<use>\\2<use>",text)

text = re.sub(lower + "[.]" + lower + "[.]"+ lower + "[.]","\\1<use>\\2<use>",text)
text = re.sub(" "+suffixes+"[.] "+starters," \\1<space> \\2",text)
text = re.sub(" "+suffixes+"[.]"," \\1<use>",text)
text = re.sub(brackets,"<space>\\1<space>",text)
text = re.sub(" " + caps + "[.]"," \\1<use>",text)


if "..." in text: text = text.replace("...","<prd><prd><prd>")
if "”" in text: text = text.replace(".”","”.")
if "\"" in text: text = text.replace(".\"","\".")
if "!" in text: text = text.replace("!\"","\"!")
if "?" in text: text = text.replace("?\"","\"?")

text = text.replace(".","<space>.<space>")
text = text.replace("?","<space>?<space>")
text = text.replace("!","<space>!<space>")
text = text.replace("-"," ")
text = text.replace(" ","<space>")
text = text.replace("<use>",".")
text = text.replace("<prd>",".")
WordTokens = text.split("<space>")
print(WordTokens)
#i=1

with open('/content/drive/My Drive/en_wiki.txt') as f:
  text = f.read()

text = " " + text + "  "
text = text.replace("\n"," ")
text = re.sub(prefixes,"\\1<use>",text)
text = re.sub(digits + "[.]" + digits,"\\1<prd>\\2",text)
text = re.sub(websites,"<use>\\1",text)
if "Ph.D" in text: text = text.replace("Ph.D.","Ph<prd>D<prd>")
text = re.sub("\s" + caps + "[.] "," \\1<use> ",text)
text = re.sub("\s" + StartingCapital + "[.] "," \\1<use> ",text)
text = re.sub("\s" + StartingCapital + "[']" + AfterAphosetrophe," \\1 ' \\2 ",text)
text = re.sub(acronyms+" "+starters,"\\1<space> \\2",text)
text = re.sub(caps + "[.]" + caps + "[.]" + caps + "[.]","\\1<use>\\2<use>\\3<use>",text)
text = re.sub(caps + "[.]" + caps + "[.]","\\1<use>\\2<use>",text)
text = re.sub(lower + "[.]" + lower + "[.]","\\1<use>\\2<use>",text)

text = re.sub(lower + "[.]" + lower + "[.]"+ lower + "[.]","\\1<use>\\2<use>",text)
text = re.sub(" "+suffixes+"[.] "+starters," \\1<space> \\2",text)
text = re.sub(" "+suffixes+"[.]"," \\1<use>",text)
text = re.sub(brackets,"<space>\\1<space>",text)
text = re.sub(" " + caps + "[.]"," \\1<use>",text)


if "..." in text: text = text.replace("...","<prd><prd><prd>")
if "”" in text: text = text.replace(".”","”.")
if "\"" in text: text = text.replace(".\"","\".")
if "!" in text: text = text.replace("!\"","\"!")
if "?" in text: text = text.replace("?\"","\"?")

text = text.replace(".","<space>.<space>")
text = text.replace("?","<space>?<space>")
text = text.replace("!","<space>!<space>")
text = text.replace("-"," ")
text = text.replace(" ","<space>")
text = text.replace("<use>",".")
text = text.replace("<prd>",".")
WordTokens = text.split("<space>")

HWordTokens = [i for i in WordTokens if i != '']

"""Stemming Process of Word Tokens found by hueristic approach"""

from nltk.stem import PorterStemmer
ps = PorterStemmer()

stemmedHuerWords = []
for w in HWordTokens:
    stemmedHuerWords.append(ps.stem(w))

Unigrams Analysis

stemH_unigrams = list(ngrams(stemmedHuerWords,1))

"""Frequency Distribution of unigrams"""

fdistStemHUni = nltk.FreqDist(stemH_unigrams)
sortedStemHUni = dict(sorted(fdistStemHUni.items(), key=operator.itemgetter(1),reverse=True))
top10StemHUni = dict(itertools.islice(sortedStemHUni.items(), 10)) 
print(top10StemHUni)

"""Least Frequent Printed"""

acensortedStemHUni = dict(sorted(fdistStemHUni.items(), key=operator.itemgetter(1)))
below10StemHUni = dict(itertools.islice(acensortedStemHUni.items(), 10)) 
print(below10StemHUni)

"""Total no. of unigrams (Distinct)"""

len(sortedStemHUni)

freqStemHUni = list(sortedStemHUni.values())

"""Total no. of unigrams (including duplicates)"""

total = 0
for val in freqStemHUni:
  total = total + val
print(total)

"""Calculating frequent required for 90% coverage"""

nintyPercent = total*(9/10)
calcu=0
count=0
for val in freqStemHUni:
  count = count +1
  calcu = calcu + val
  if calcu>nintyPercent:
    break
print(count)

binu = [1,2,3,4,5,6,7,8,9,10,15,20,25,30,35,40,50]
plt.hist(freqStemHUni,bins=binu,edgecolor='black')

"""Bigram Analysis"""

stemH_bigrams = list(ngrams(stemmedHuerWords,2))

"""Frequency Distribution"""

fdistStemHBi = nltk.FreqDist(stemH_bigrams)
sortedStemHBi = dict(sorted(fdistStemHBi.items(), key=operator.itemgetter(1),reverse=True))
top10StemHBi = dict(itertools.islice(sortedStemHBi.items(), 10)) 
print(top10StemHBi)

"""Least Frequent Printed"""

acensortedStemHBi = dict(sorted(fdistStemHBi.items(), key=operator.itemgetter(1)))
below10StemHBi = dict(itertools.islice(acensortedStemHBi.items(), 10)) 
print(below10StemHBi)

"""TOtal distinct bigrams"""

len(sortedStemHBi)

freqStemHBi = list(sortedStemHBi.values())

"""Total bigrams (Duplicate included)"""

total = 0
for val in freqStemHBi:
  total = total + val
print(total)

"""Frequent Bigrams for 80% coverage"""

eightyPercent = total*(8/10)
calcu=0
count=0
for val in freqStemHBi:
  count = count +1
  calcu = calcu + val
  if calcu>eightyPercent:
    break
print(count)

binb = [1,2,3,4,5,6,7,8,9,10,15,20,25,30,35,40,50]
plt.hist(freqStemHBi,bins=binb,edgecolor='black')

Trigram Analysis

stemH_trigrams = list(ngrams(stemmedHuerWords,3))

"""Frequency Distribution of Trigrams"""

fdistStemHTri = nltk.FreqDist(stemH_trigrams)
sortedStemHTri = dict(sorted(fdistStemHTri.items(), key=operator.itemgetter(1),reverse=True))
top10StemHTri = dict(itertools.islice(sortedStemHTri.items(), 10)) 
print(top10StemHTri)

"""Least Frequent Trigrams Printed"""

acensortedStemHTri = dict(sorted(fdistStemHTri.items(), key=operator.itemgetter(1)))
below10StemHTri = dict(itertools.islice(acensortedStemHTri.items(), 10)) 
print(below10StemHTri)

"""No of Distinct Trigrams"""

len(sortedStemHTri)

"""Extracting only frequency"""

freqStemHTri = list(sortedStemHTri.values())

"""Total no. Trigrams (Duplicate Included)"""

total = 0
for val in freqStemHTri:
  total = total + val
print(total)

"""Frequent Required for 70% coverage"""

seventyPercent = total*(7/10)
calcu=0
count=0
for val in freqStemHTri:
  count = count +1
  calcu = calcu + val
  if calcu>seventyPercent:
    break
print(count)

bint = [1,2,3,4,5,6,7,8,9,10,15,20,25,30,35,40,50]
plt.hist(freqStemHTri,bins=bint,edgecolor='black')

"""**Likelilihood Ratio Calculation for bigrams**

SortedStemUni is the dictionary containing unigrams and their frequency taken from above.
SortedStemBi is the dictionary containing bigrams and their frequency taken from above.
Total no. of Bigrams = Total no. of unigrams = 19602235 (taken from above)

For Testing purpose
"""

thisdict = {('the','of'): 1,('of','the'): 1}
unidict = {('the',): 5,('of',): 20}
N = 1000

"""Libraries needed for maths"""

from scipy.stats import binom 
import math
from decimal import *

likelihoodRatio = {}
for x, y in thisdict.items():
  print(x, y)
  w1 = x[0]
  w2 = x[1]
  c12 = y
  c1 = unidict[(w1,)]
  c2 = unidict[(w2,)]
  print(c1,c2,c12)
  p= c2/N
  p1 = c12/c1
  p2 = (c2-c12)/(N-c1)
  print(p,p1,p2)
  getcontext().prec = 50
  b11 = binom.pmf(c12, c1, p)
  b12 = binom.pmf(c2-c12, N-c1, p)
  b21 = binom.pmf(c12, c1, p1)
  b22 = binom.pmf(c2-c12, N-c1, p2)
  print(b11,b12,b21,b22)
  lgL = (math.log(b11) + math.log(b12))-(math.log(b21) + math.log(b22))
  ans = 2*(-1)*lgL
  print(ans)
  likelihoodRatio[x] =ans
print(likelihoodRatio)

"""Calculating For all bigrams"""

N = 19602235
likelihoodRatio = {}
for x, y in sortedStemBi.items():
  #print(x, y)
  w1 = x[0]
  w2 = x[1]
  c12 = y
  c1 = sortedStemUni[(w1,)]
  c2 = sortedStemUni[(w2,)]
  #print(c1,c2,c12)
  p= c2/N
  p1 = c12/c1
  p2 = (c2-c12)/(N-c1)
  #print(p,p1,p2)
  getcontext().prec = 100
  b11 = binom.logpmf(c12, c1, p)
  b12 = binom.logpmf(c2-c12, N-c1, p)
  b21 = binom.logpmf(c12, c1, p1)
  b22 = binom.logpmf(c2-c12, N-c1, p2)
  #print(b11,b12,b21,b22)
  lgL = (b11 + b12)-(b21 + b22)
  ans = 2*(-1)*lgL
  #print(ans)
  likelihoodRatio[x] =ans
#print(likelihoodRatio)

sortedLikelihood = dict(sorted(likelihoodRatio.items(), key=operator.itemgetter(1),reverse=True))
top100likelihood = dict(itertools.islice(sortedLikelihood.items(), 100))
print(top100likelihood)

print(len(sortedLikelihood))

ascesortedLikelihood = dict(sorted(likelihoodRatio.items(), key=operator.itemgetter(1)))
below100likelihood = dict(itertools.islice(ascesortedLikelihood.items(), 100))
print(below100likelihood)

"""Morphological Analysis

Calculating Most frequent words
"""

fdistWords = nltk.FreqDist(wordTokens1)
sortedWords = dict(sorted(fdistWords.items(), key=operator.itemgetter(1),reverse=True))
top100Words = dict(itertools.islice(sortedWords.items(), 100)) 
print(top100Words)

top500Words = dict(itertools.islice(sortedWords.items(), 500))

list500Words = list(top500Words.keys())

"""**Filtering Words because most frequent words are preposition, punctuations, cunjunctions and they does not contain meaning when divided**"""

wordWithTags = nltk.pos_tag(list500Words)
print(wordWithTags)

filteredWords = [t for t in wordWithTags if (t[1] == "NNP" or t[1] == "RB" or t[1] == "JJ" or t[1] == "VBN" or t[1] == "VBP" or t[1] == "NNS" or t[1] == "NNPS" or t[1] == "NN" or t[1] == "VBG" or t[1] == "VBZ" or t[1] == "JJR" or t[1] == "VBD")]

print(filteredWords)

len(filteredWords)

list100FilteredWords = []
for i in filteredWords:
  list100FilteredWords.append(i[0])

print(list100FilteredWords)

"""Randomly Selecting from those 100 words"""

import random

random5Words  = random.sample(list100FilteredWords, 5)

print(random5Words)

"""**Installing Polyglot for morphologicl analysis**"""

!pip install polyglot

!apt install python-numpy libicu-dev
!apt install python3-icu

!pip3 install pycld2
from polyglot.downloader import downloader
print(downloader.supported_languages_table("morph2"))

!pip3 install morfessor

!polyglot download morph2.en morph2.ar

from polyglot.text import Text, Word

"""Morphological Analysis of Most Frequent Words"""

for w in random5Words:
  w = Word(w, language="en")
  print("{:<20}{}".format(w, w.morphemes))

"""Morphological Analysis of Least Frequent Words"""

asceSortedWords = dict(sorted(fdistWords.items(), key=operator.itemgetter(1)))
below100Words = dict(itertools.islice(asceSortedWords.items(), 100)) 
print(below100Words)

list100BelowWords = list(below100Words.keys())

random5BelowWords  = random.sample(list100BelowWords, 5)

for w in random5BelowWords:
  w = Word(w, language="en")
  print("{:<20}{}".format(w, w.morphemes))

"""**Byte Pair Encoding**"""

import re, collections
from IPython.display import display, Markdown, Latex

"""Taking Small Data set for training"""

vocab = collections.defaultdict(int)
with open('/content/drive/My Drive/BPEenglish.txt', 'r', encoding='utf-8') as fhand:
    for line in fhand:
        words = line.strip().split()
        for word in words:
            vocab[' '.join(list(word)) + ' </w>'] += 1

def get_stats(vocab):
    """Compute frequencies of adjacent pairs of symbols."""
    pairs = collections.defaultdict(int)
    for word, freq in vocab.items():
        symbols = word.split()
        for i in range(len(symbols)-1):
            pairs[symbols[i],symbols[i+1]] += freq
    return pairs

def merge_vocab(pair, v_in):
    v_out = {}
    bigram = re.escape(' '.join(pair))
    p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
    for word in v_in:
        w_out = p.sub(''.join(pair), word)
        v_out[w_out] = v_in[word]
    return v_out

bpe_codes = {}
bpe_codes_reverse = {}

num_merges = 10000

for i in range(num_merges):
    pairs = get_stats(vocab)
    if not pairs:
        break
    best = max(pairs,key=pairs.get)
    vocab = merge_vocab(best, vocab)
    
    bpe_codes[best] = i
    bpe_codes_reverse[best[0] + best[1]] = best

tokens = {}
for word, freq in vocab.items():
    word_tokens = word.split()
    for token in word_tokens:
      if token in tokens.keys():
        tokens[token] += freq
      else:
        tokens[token] = 1

stripped = {}
for token,frequ in tokens.items():
  if (token[-4:]=='</w>'):
    stripped[token[:-4]] = frequ
  else:
    stripped[token] = frequ

tokens = stripped

sortedTokens = dict(sorted(tokens.items(), key=operator.itemgetter(1), reverse= True))
top100Tokens = dict(itertools.islice(sortedTokens.items(), 100))

import random

randomTop5Words  = random.sample(list(top100Tokens.keys()), 5)

print(randomTop5Words)

asceSortedTokens = dict(sorted(tokens.items(), key=operator.itemgetter(1)))
below100Tokens = dict(itertools.islice(asceSortedTokens.items(), 100))

randomBelow5Words  = random.sample(list(below100Tokens.keys()), 5)

print(randomBelow5Words)

def get_pairs(word):    
    pairs = set()
    prev_char = word[0]
    for char in word[1:]:
        pairs.add((prev_char, char))
        prev_char = char
    return pairs


def encode(orig):
    word = tuple(orig) + ('</w>',)

    pairs = get_pairs(word)    

    if not pairs:
        return orig

    iteration = 0
    while True:
        iteration += 1
        
        bigram = min(pairs, key = lambda pair: bpe_codes.get(pair, float('inf')))
        ans = bigram
        if bigram not in bpe_codes:
            break
        first, second = bigram
        new_word = []
        i = 0
        while i < len(word):
            try:
                j = word.index(first, i)
                new_word.extend(word[i:j])
                i = j
            except:
                new_word.extend(word[i:])
                break

            if word[i] == first and i < len(word)-1 and word[i+1] == second:
                new_word.append(first+second)
                i += 2
            else:
                new_word.append(word[i])
                i += 1
        new_word = tuple(new_word)
        word = new_word
        if len(word) == 1:
            break
        else:
            pairs = get_pairs(word)

    # don't print end-of-word symbols
    if word[-1] == '</w>':
        word = word[:-1]
    elif word[-1].endswith('</w>'):
        word = word[:-1] + (word[-1].replace('</w>',''),)
   
    return ans
WordsNotInCorpora = ["ramification", "benzaldehyde", "provethem","cusotmer", "illustrator", "photoshop", "internship", "automatically", "indivisually", "hexagonal"]
for orig in WordsNotInCorpora:
  ans = encode(orig)
  listi = []
  listi.append(ans[0])
  listi.append(ans[1][:-4])
  print("{}      {}".format(orig,listi))

print("5 Random words for most frequent category: -")
for orig in randomTop5Words:
  ans = encode(orig)
  listi = []
  listi.append(ans[0])
  listi.append(ans[1][:-4])
  print("{}      {}".format(orig,listi)) 
print("")
print("")
print("5 Random words for least frequent category: -")
for orig in randomBelow5Words:
  ans = encode(orig)
  listi = []
  listi.append(ans[0])
  listi.append(ans[1][:-4])
  print("{}      {}".format(orig,listi))