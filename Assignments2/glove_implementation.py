# -*- coding: utf-8 -*-
"""GloVE_Implementation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13wiKCr-hc2iwmK6qcu7zYfk-V_tLrZEU
"""

!pip install nltk

import nltk
nltk.download('popular')

from nltk.tokenize import sent_tokenize, word_tokenize

with open('/content/drive/My Drive/shortestCorpus.txt') as f:
  text = f.read()

sentenceTokens = sent_tokenize(text)
print(len(sentenceTokens))
sentenceTokens = sentenceTokens[2000:3000]

import itertools
import operator
import collections
from collections import Counter
import matplotlib.pyplot as plt
from nltk.util import ngrams
from scipy.stats import binom 
import math
from decimal import *
import random
import _pickle as pickle
import msgpack
import numpy as np
from scipy import sparse
from nltk.probability import FreqDist

"""Function For Tokenizing all sentences"""

def tokenizeAllSentences(listOfSen):
  allWordTokens = []
  for sen in listOfSen:
    allWordTokens.extend(word_tokenize(sen))
  return allWordTokens

"""Function for making dictionary from list"""

def listToDict(wordTokensTrain):
  freqWords = dict(Counter(wordTokensTrain))
  return freqWords

"""Fucntion for converting dictionary into new form dict[words] = (wordId,freq)"""

def transformingDict(freqWords):
  listOfKeys = list(freqWords.keys())
  transformed = {}
  for i in range(len(listOfKeys)):
    freq = freqWords[listOfKeys[i]]
    transformed[listOfKeys[i]] = (i,freq)
  return transformed

"""Function For making dictionary of index-word mapping"""

def indextoWordDict(freqWords):
  indexWordDict = {}
  for val in freqWords:
    temp = freqWords[val]
    indexWordDict[temp[0]] = val
  return indexWordDict

"""Function For Making coOccurance Matrix for vocabulary"""

def makeCoOccuranceMatrix(indexWordDict,freqWords,sentenceTokens,windowSize=10,minFreq = 4):
  #indexWordDict = indextoWordDict(freqWords)
  coOccurrenceMatrix = sparse.lil_matrix((len(freqWords), len(freqWords)),dtype=np.float64)
  for i, sen in enumerate(sentenceTokens):
    listSenTokens = word_tokenize(sen)
    #print(listSenTokens)
    tokenIndex = [freqWords[word][0] for word in listSenTokens]

    for ci, centerIndex in enumerate(tokenIndex):
      contextIndexs = tokenIndex[max(0, ci - windowSize) : ci]
      contextLen = len(contextIndexs)

      for li, leftIndex in enumerate(contextIndexs):
        dist = contextLen - li

        inc = 1.0 / float(dist)

        coOccurrenceMatrix[centerIndex, leftIndex] += inc
        coOccurrenceMatrix[leftIndex, centerIndex] += inc

  for i, (row, data) in enumerate(zip(coOccurrenceMatrix.rows,coOccurrenceMatrix.data)):
    if freqWords[indexWordDict[i]][1] < minFreq:
      continue

    for dataIndex, j in enumerate(row):
      if freqWords[indexWordDict[j]][1] < minFreq:
        continue
      yield i, j, data[dataIndex]

"""Function For Iteratively Calculating Cost"""

def run_iter(freqWords, coOccurrenceMatrix, data, alph=0.75, learningRate=0.05, xMax=100):
      
  totalCost = 0
  vocabLen = len(freqWords)
  #print(vocabLen)
  for i, j, coOccurrence in coOccurrenceMatrix:

    wordi = data[0][i]
    wordj = data[0][j+vocabLen]
    biasi = data[1][i]
    biasj = data[1][j+vocabLen]

    weight = (coOccurrence / xMax) ** alph if coOccurrence < xMax else 1
    innerCost = (wordi.dot(wordj) + biasi + biasj - math.log(coOccurrence)) 
    cost = weight * (innerCost ** 2)
    totalCost += 0.5 * cost 

    gradWordi = weight * innerCost * wordj
    gradWordj = weight * innerCost * wordi
    gradBiasi = weight * innerCost
    gradBiasj = weight * innerCost

    sqGradWordi = data[2][i]
    sqGradWordj = data[2][j+vocabLen]
    sqGradBiasi = data[3][i]
    sqGradBiasj = data[3][j+vocabLen]

    wordi -= (learningRate * gradWordi / np.sqrt(sqGradWordi))
    wordj -= (learningRate * gradWordj / np.sqrt(sqGradWordj))
    biasi -= (learningRate * gradBiasi / math.sqrt(sqGradBiasi))
    biasj -= (learningRate * gradBiasj / math.sqrt(sqGradBiasj))
    
    biasi = 0
    sqGradWordi += np.square(gradWordi)
    sqGradWordj += np.square(gradWordj)
    sqGradBiasi += gradBiasi ** 2
    sqGradBiasj += gradBiasj ** 2

    data[0][i] = wordi
    data[0][j+vocabLen] = wordj 
    data[1][i] = biasi
    data[1][j+vocabLen] = biasj  
    data[2][i] = sqGradWordi 
    data[2][j+vocabLen] = sqGradWordj 
    data[3][i] = sqGradBiasi 
    data[3][j+vocabLen] = sqGradBiasj

  return totalCost

"""Function For Calculating Word Embedding"""

def trainGlove(freqWords, coOccurrenceMatrix, vector_size=50,iterations=25, **kwargs):
    
  vocabLen = len(freqWords)
  np.random.seed(0)
  wordEmbedding = (np.random.rand(vocabLen * 2, vector_size) - 0.5) / float(vector_size + 1)
  biases = (np.random.rand(vocabLen * 2) - 0.5) / float(vector_size + 1)
  sqGrad = np.ones((vocabLen * 2, vector_size),dtype=np.float64)
  sqGradbiases = np.ones(vocabLen * 2, dtype=np.float64)
  data = (wordEmbedding,biases,sqGrad,sqGradbiases)

  for i in range(iterations):
    cost = run_iter(freqWords,coOccurrenceMatrix, data, **kwargs)

  return data[0]

wordTokensTrain = tokenizeAllSentences(sentenceTokens)

freqWords = listToDict(wordTokensTrain)

print(len(freqWords))

freqWords = transformingDict(freqWords)

indexWordDict = indextoWordDict(freqWords)

coOccuranceMatrix = list(makeCoOccuranceMatrix(indexWordDict,freqWords,sentenceTokens))

print(coOccuranceMatrix)

print(indexWordDict[0]," ",indexWordDict[2])

wordEmbedding = trainGlove(freqWords, CoOccuranceMatrix)

wordEmbedding = wordEmbedding[:len(freqWords), :]
for i, row in enumerate(wordEmbedding):
    wordEmbedding[i, :] /= np.linalg.norm(row)

wordEmbedding[:100]

"""For Finding Most Similar Words"""

def mostSimilar(positive, negative, topn=10, freq_threshold=4):
  mean_vecs = []
  for word in positive: mean_vecs.append(wordEmbedding[freqWords[word][0]])
  for word in negative: mean_vecs.append(-1 * wordEmbedding[freqWords[word][0]])
  
  mean = np.array(mean_vecs).mean(axis=0)
  mean /= np.linalg.norm(mean)
  dists = np.dot(wordEmbedding, mean)
  best = np.argsort(dists)[::-1][:topn + len(positive) + len(negative) + 100]
  
  result = [(indexWordDict[i], dists[i]) for i in best if (freqWords[indexWordDict[i]][1] >= freq_threshold and indexWordDict[i] not in positive and indexWordDict[i] not in negative)]
  return result[:topn]

print(mostSimilar(['two'], ['one']))
print(mostSimilar(['was', 'has'], ['is']))
print(mostSimilar(['by', 'the'], ['to']))
print(mostSimilar(['husband', 'father'], ['wife']))

def read_pretrained_data(file_name):
  with open(file_name,'r') as f:
    vocab = FreqDist()
    word2vector = []
    for line in f:
      words_Vec = line.split()
      tok = list()
      tok.append(words_Vec[0])
      vocab.update(tok)
      word2vector.append(np.array(words_Vec[1:],dtype=float))
  return ({word: (i, freq) for i, (word, freq) in enumerate(vocab.items())},word2vector)

preTrainedVoc, preTrainedEmbed = read_pretrained_data('/content/drive/My Drive/glove.6B.50d.txt')

preTrainedEmbed[:100]

print((list(preTainedVoc.keys()))[:100])

print(preTrainedEmbed[preTrainedVoc['the'][0]])

preIndexWordDict = indextoWordDict(preTrainedVoc)

def mostPreSimilar(positive, negative, topn=10, freq_threshold=1):
  mean_vecs = []
  for word in positive: mean_vecs.append(preTrainedEmbed[preTrainedVoc[word][0]])
  for word in negative: mean_vecs.append(-1 * preTrainedEmbed[preTrainedVoc[word][0]])
  
  mean = np.array(mean_vecs).mean(axis=0)
  mean /= np.linalg.norm(mean)
  dists = np.dot(preTrainedEmbed, mean)
  best = np.argsort(dists)[::-1][:topn + len(positive) + len(negative) + 100]
  
  result = [(preIndexWordDict[i], dists[i]) for i in best if (preTrainedVoc[preIndexWordDict[i]][1] >= freq_threshold and preIndexWordDict[i] not in positive and preIndexWordDict[i] not in negative)]
  return result

print(mostPreSimilar(['two'], ['one']))
print(mostPreSimilar(['was', 'has'], ['is']))
print(mostPreSimilar(['by', 'the'], ['to']))
print(mostPreSimilar(['husband', 'father'], ['wife']))